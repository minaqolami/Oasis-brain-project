{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random \n",
    "import numpy.random\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import scipy.special\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orginal_data = pd.read_csv(\"F:\\\\abc.csv\")\n",
    "mu_eTIV, sigma_eTIV = orginal_data['eTIV'].mean(), orginal_data['eTIV'].var()\n",
    "mu_MMSE, sigma_MMSE = orginal_data['MMSE'].mean(), orginal_data['MMSE'].var()\n",
    "mu_EDUC, sigma_EDUC = orginal_data['EDUC'].mean(), orginal_data['EDUC'].var()\n",
    "age_range = (60, 100)\n",
    "\n",
    "def build_lstm_rnn(input_shape):\n",
    "    lstm_rnn = tf.keras.Sequential()\n",
    "    lstm_rnn.add(tf.keras.layers.LSTM(100, return_sequences=True, input_shape=input_shape))\n",
    "    lstm_rnn.add(tf.keras.layers.LSTM(50, return_sequences=False))\n",
    "    lstm_rnn.add(tf.keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "    lstm_rnn.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    return lstm_rnn\n",
    "\n",
    "accuracy_list = []\n",
    "recall_list = []\n",
    "precision_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_18224\\2871003694.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"Age\"] = age_scaler.transform(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_18224\\2871003694.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"EDUC\"] = educ_scaler.transform(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_18224\\2871003694.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"MMSE\"] = mmse_scaler.transform(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_18224\\2871003694.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"eTIV\"] = etiv_scaler.transform(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_18224\\2871003694.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"Age\"] = age_scaler.transform(X_test[\"Age\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_18224\\2871003694.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"EDUC\"] = educ_scaler.transform(X_test[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_18224\\2871003694.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"MMSE\"] = mmse_scaler.transform(X_test[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_18224\\2871003694.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"eTIV\"] = mmse_scaler.transform(X_test[\"eTIV\"].to_numpy().reshape(-1, 1))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 80\n  y sizes: 400\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m X_test_super_final \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(X_test_final\u001b[39m.\u001b[39mto_numpy(),(X_test_final\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m,X_train_final\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],rows_n))\n\u001b[0;32m     93\u001b[0m lstm_rnn \u001b[39m=\u001b[39m build_lstm_rnn((X_train_super_final\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],X_train_super_final\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]) )\n\u001b[1;32m---> 94\u001b[0m lstm_rnn\u001b[39m.\u001b[39;49mfit(X_train_super_final, y_train_super_final, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m)\n\u001b[0;32m     95\u001b[0m not_final_lstm_rnn_prediction \u001b[39m=\u001b[39m lstm_rnn\u001b[39m.\u001b[39mpredict(X_test_super_final)\n\u001b[0;32m     96\u001b[0m lstm_rnn_prediction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(not_final_lstm_rnn_prediction \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1848\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1842\u001b[0m         label,\n\u001b[0;32m   1843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m   1844\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1845\u001b[0m         ),\n\u001b[0;32m   1846\u001b[0m     )\n\u001b[0;32m   1847\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1848\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 80\n  y sizes: 400\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    N = 100\n",
    "    m = 5\n",
    "\n",
    "    Y = eta = np.zeros((N, m))\n",
    "    Age = np.random.choice(range(age_range[0], age_range[1] + 1), size=N)\n",
    "    MMSE = np.round(np.random.normal(mu_MMSE, sigma_MMSE, size=N))\n",
    "    EDUC = np.round(np.random.normal(mu_EDUC, sigma_EDUC, size=N))\n",
    "    Sex = np.random.binomial(1, 0.2, size=N)\n",
    "    eTIV = np.round(np.random.normal(mu_eTIV, sigma_eTIV, size=N))\n",
    "    ASF = np.random.normal(size=N)\n",
    "    nWBV = np.random.normal(size=N)\n",
    "    Visit = np.round(np.linspace(0, 4, num=m))\n",
    "\n",
    "    B = np.array([0.1, -0.1, 0.2, -0.2, 0.3, 0.3, 0.1, 0.1, 0.2, 0.1])\n",
    "    b = np.random.normal(0, 0.5, size=N)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(m):\n",
    "            eta[i, j] = B[0] + B[1] * Visit[j] + B[2] * Age[i] + B[3] * MMSE[i] + B[4] * EDUC[i] + B[5] * Sex[i] + \\\n",
    "                        B[6] * eTIV[i] + B[7] * ASF[i] + B[8] * nWBV[i] + b[i]\n",
    "            p = scipy.special.expit(eta[i, j])\n",
    "            Y[i, j] = np.random.binomial(1, p)\n",
    "\n",
    "   \n",
    "\n",
    "    mat = np.empty((N * m, 10))\n",
    "    p = 0\n",
    "    for i in range(N):\n",
    "        jk = 0\n",
    "        for jj in range(p, p + m):\n",
    "            mat[jj, 0] = i + 1\n",
    "            mat[jj, 1] = Visit[jk]\n",
    "            mat[jj, 2] = Age[i]\n",
    "            mat[jj, 3] = MMSE[i]\n",
    "            mat[jj, 4] = EDUC[i]\n",
    "            mat[jj, 5] = Sex[i]\n",
    "            mat[jj, 6] = eTIV[jk]\n",
    "            mat[jj, 7] = ASF[jk]\n",
    "            mat[jj, 8] = nWBV[jk]\n",
    "            mat[jj, 9] = Y[i, jk]\n",
    "            jk += 1\n",
    "        p += m\n",
    "\n",
    "    # Convert the `mat` array to a DataFrame\n",
    "    df_mat = pd.DataFrame(mat, columns=['ID', 'Visit', 'Age', 'MMSE', 'EDUC', 'Sex', 'eTIV', 'ASF', 'nWBV', 'Y'])\n",
    "    df = pd.get_dummies(df_mat, columns=[\"Visit\"], prefix=\"Visit\")\n",
    "    RANDOM_STATE = 13\n",
    "    splitter = GroupShuffleSplit(test_size=.20, n_splits=1, random_state=RANDOM_STATE)\n",
    "    split = splitter.split(df, groups=df['ID'])\n",
    "    train_indexes, test_indexes = next(split)\n",
    "\n",
    "    X_train = df.iloc[train_indexes]\n",
    "    X_test = df.iloc[test_indexes]\n",
    "    age_scaler = MinMaxScaler()\n",
    "    educ_scaler = MinMaxScaler()\n",
    "    mmse_scaler = MinMaxScaler()\n",
    "    etiv_scaler = MinMaxScaler()\n",
    "\n",
    "    age_scaler.fit(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
    "    educ_scaler.fit(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
    "    mmse_scaler.fit(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
    "    etiv_scaler.fit(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    X_train[\"Age\"] = age_scaler.transform(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
    "    X_train[\"EDUC\"] = educ_scaler.transform(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
    "    X_train[\"MMSE\"] = mmse_scaler.transform(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
    "    X_train[\"eTIV\"] = etiv_scaler.transform(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    X_test[\"Age\"] = age_scaler.transform(X_test[\"Age\"].to_numpy().reshape(-1, 1))\n",
    "    X_test[\"EDUC\"] = educ_scaler.transform(X_test[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
    "    X_test[\"MMSE\"] = mmse_scaler.transform(X_test[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
    "    X_test[\"eTIV\"] = mmse_scaler.transform(X_test[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
    "    visit_3 = df[df[\"Visit_4.0\"] == 1][[\"ID\", \"Y\"]]\n",
    "    y_train_final = []\n",
    "    for id in X_train[\"ID\"]:\n",
    "        y_train_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
    "\n",
    "    y_test_final = []\n",
    "    for id in X_test[\"ID\"]:\n",
    "        y_test_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
    "\n",
    "    y_train_super_final = pd.DataFrame({\n",
    "    \"Group\": y_train_final})\n",
    "    y_test_super_final = pd.DataFrame({\n",
    "    \"Group\": y_test_final})\n",
    "    X_test_final = X_test.drop([\"ID\"], axis = 1)\n",
    "    X_train_final = X_train.drop([\"ID\"], axis = 1)\n",
    "    rows_n = 5\n",
    "\n",
    "    X_train_super_final = np.reshape(X_train_final.to_numpy(),(X_train_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
    "    X_test_super_final = np.reshape(X_test_final.to_numpy(),(X_test_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
    "    lstm_rnn = build_lstm_rnn((X_train_super_final.shape[1],X_train_super_final.shape[2]) )\n",
    "    lstm_rnn.fit(X_train_super_final, y_train_super_final, epochs = 100)\n",
    "    not_final_lstm_rnn_prediction = lstm_rnn.predict(X_test_super_final)\n",
    "    lstm_rnn_prediction = np.where(not_final_lstm_rnn_prediction > 0.5, 1, 0)\n",
    "    lstm_rnn_conf = confusion_matrix(y_test_super_final,lstm_rnn_prediction)\n",
    "    lstm_rnn_plot_conf = ConfusionMatrixDisplay(lstm_rnn_conf)\n",
    "    lstm_rnn_plot_conf.plot()\n",
    "    TP=lstm_rnn_conf[1,1]\n",
    "    FP=lstm_rnn_conf[0,1]\n",
    "    TN=lstm_rnn_conf[0,0]\n",
    "    FN=lstm_rnn_conf[1,0]\n",
    "    accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
    "    recall =TP/(TP+FN)\n",
    "    precision =TP/(TP+TN)\n",
    "    accuracy_list.append(accuracy)\n",
    "    recall_list.append(recall)\n",
    "    precision_list.append(precision)\n",
    "\n",
    "print(\"Accuracy List:\", accuracy_list)\n",
    "print(\"Recall List:\", recall_list)\n",
    "print(\"Precision List:\", precision_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13284\\2922596249.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"Age\"] = age_scaler.transform(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13284\\2922596249.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"EDUC\"] = educ_scaler.transform(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13284\\2922596249.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"MMSE\"] = mmse_scaler.transform(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13284\\2922596249.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[\"eTIV\"] = etiv_scaler.transform(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13284\\2922596249.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"Age\"] = age_scaler.transform(X_test[\"Age\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13284\\2922596249.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"EDUC\"] = educ_scaler.transform(X_test[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13284\\2922596249.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"MMSE\"] = mmse_scaler.transform(X_test[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_13284\\2922596249.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[\"eTIV\"] = mmse_scaler.transform(X_test[\"eTIV\"].to_numpy().reshape(-1, 1))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 80\n  y sizes: 400\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 93\u001b[0m\n\u001b[0;32m     91\u001b[0m X_test_super_final \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(X_test_final\u001b[39m.\u001b[39mto_numpy(),(X_test_final\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m5\u001b[39m,X_train_final\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],rows_n))\n\u001b[0;32m     92\u001b[0m lstm_rnn \u001b[39m=\u001b[39m build_lstm_rnn((X_train_super_final\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],X_train_super_final\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m]) )\n\u001b[1;32m---> 93\u001b[0m lstm_rnn\u001b[39m.\u001b[39;49mfit(X_train_super_final, y_train_super_final, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m)\n\u001b[0;32m     94\u001b[0m not_final_lstm_rnn_prediction \u001b[39m=\u001b[39m lstm_rnn\u001b[39m.\u001b[39mpredict(X_test_super_final)\n\u001b[0;32m     95\u001b[0m lstm_rnn_prediction \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(not_final_lstm_rnn_prediction \u001b[39m>\u001b[39m \u001b[39m0.5\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\data_adapter.py:1848\u001b[0m, in \u001b[0;36m_check_data_cardinality\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m  \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m sizes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1842\u001b[0m         label,\n\u001b[0;32m   1843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[0;32m   1844\u001b[0m             \u001b[39mstr\u001b[39m(i\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten(single_data)\n\u001b[0;32m   1845\u001b[0m         ),\n\u001b[0;32m   1846\u001b[0m     )\n\u001b[0;32m   1847\u001b[0m msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mMake sure all arrays contain the same number of samples.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1848\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 80\n  y sizes: 400\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    N = 100\n",
    "    m = 5\n",
    "\n",
    "    Y = eta = np.zeros((N, m))\n",
    "    Age = np.random.choice(range(age_range[0], age_range[1] + 1), size=N)\n",
    "    MMSE = np.round(np.random.normal(mu_MMSE, sigma_MMSE, size=N))\n",
    "    EDUC = np.round(np.random.normal(mu_EDUC, sigma_EDUC, size=N))\n",
    "    Sex = np.random.binomial(1, 0.2, size=N)\n",
    "    eTIV = np.round(np.random.normal(mu_eTIV, sigma_eTIV, size=N))\n",
    "    ASF = np.random.normal(size=N)\n",
    "    nWBV = np.random.normal(size=N)\n",
    "    Visit = np.round(np.linspace(0, 4, num=m))\n",
    "\n",
    "    B = np.array([0.1, -0.1, 0.2, -0.2, 0.3, 0.3, 0.1, 0.1, 0.2, 0.1])\n",
    "    b = np.random.normal(0, 0.5, size=N)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(m):\n",
    "            eta[i, j] = B[0] + B[1] * Visit[j] + B[2] * Age[i] + B[3] * MMSE[i] + B[4] * EDUC[i] + B[5] * Sex[i] + \\\n",
    "                        B[6] * eTIV[i] + B[7] * ASF[i] + B[8] * nWBV[i] + b[i]\n",
    "            p = scipy.special.expit(eta[i, j])\n",
    "            Y[i, j] = np.random.binomial(1, p)\n",
    "\n",
    "    np.random.seed(1)\n",
    "\n",
    "    mat = np.empty((N * m, 10))\n",
    "    p = 0\n",
    "    for i in range(N):\n",
    "        jk = 0\n",
    "        for jj in range(p, p + m):\n",
    "            mat[jj, 0] = i + 1\n",
    "            mat[jj, 1] = Visit[jk]\n",
    "            mat[jj, 2] = Age[i]\n",
    "            mat[jj, 3] = MMSE[i]\n",
    "            mat[jj, 4] = EDUC[i]\n",
    "            mat[jj, 5] = Sex[i]\n",
    "            mat[jj, 6] = eTIV[jk]\n",
    "            mat[jj, 7] = ASF[jk]\n",
    "            mat[jj, 8] = nWBV[jk]\n",
    "            mat[jj, 9] = Y[i, jk]\n",
    "            jk += 1\n",
    "        p += m\n",
    "\n",
    "    # Convert the `mat` array to a DataFrame\n",
    "    df_mat = pd.DataFrame(mat, columns=['ID', 'Visit', 'Age', 'MMSE', 'EDUC', 'Sex', 'eTIV', 'ASF', 'nWBV', 'Y'])\n",
    "    df = pd.get_dummies(df_mat, columns=[\"Visit\"], prefix=\"Visit\")\n",
    "    RANDOM_STATE = 13\n",
    "    splitter = GroupShuffleSplit(test_size=.20, n_splits=1, random_state=RANDOM_STATE)\n",
    "    split = splitter.split(df, groups=df['ID'])\n",
    "    train_indexes, test_indexes = next(split)\n",
    "\n",
    "    X_train = df.iloc[train_indexes]\n",
    "    X_test = df.iloc[test_indexes]\n",
    "    age_scaler = MinMaxScaler()\n",
    "    educ_scaler = MinMaxScaler()\n",
    "    mmse_scaler = MinMaxScaler()\n",
    "    etiv_scaler = MinMaxScaler()\n",
    "\n",
    "    age_scaler.fit(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
    "    educ_scaler.fit(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
    "    mmse_scaler.fit(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
    "    etiv_scaler.fit(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    X_train[\"Age\"] = age_scaler.transform(X_train[\"Age\"].to_numpy().reshape(-1, 1))\n",
    "    X_train[\"EDUC\"] = educ_scaler.transform(X_train[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
    "    X_train[\"MMSE\"] = mmse_scaler.transform(X_train[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
    "    X_train[\"eTIV\"] = etiv_scaler.transform(X_train[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "    X_test[\"Age\"] = age_scaler.transform(X_test[\"Age\"].to_numpy().reshape(-1, 1))\n",
    "    X_test[\"EDUC\"] = educ_scaler.transform(X_test[\"EDUC\"].to_numpy().reshape(-1, 1))\n",
    "    X_test[\"MMSE\"] = mmse_scaler.transform(X_test[\"MMSE\"].to_numpy().reshape(-1, 1))\n",
    "    X_test[\"eTIV\"] = mmse_scaler.transform(X_test[\"eTIV\"].to_numpy().reshape(-1, 1))\n",
    "    visit_3 = df[df[\"Visit_4.0\"] == 1][[\"ID\", \"Y\"]]\n",
    "    y_train_final = []\n",
    "    for id in X_train[\"ID\"]:\n",
    "        y_train_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
    "\n",
    "    y_test_final = []\n",
    "    for id in X_test[\"ID\"]:\n",
    "        y_test_final.append(visit_3[visit_3[\"ID\"] == id][\"Y\"].values[0])\n",
    "\n",
    "    y_train_super_final = pd.DataFrame({\n",
    "    \"Group\": y_train_final})\n",
    "    y_test_super_final = pd.DataFrame({\n",
    "    \"Group\": y_test_final})\n",
    "    X_test_final = X_test.drop([\"ID\"], axis = 1)\n",
    "    X_train_final = X_train.drop([\"ID\"], axis = 1)\n",
    "    rows_n = 5\n",
    "\n",
    "    X_train_super_final = np.reshape(X_train_final.to_numpy(),(X_train_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
    "    X_test_super_final = np.reshape(X_test_final.to_numpy(),(X_test_final.shape[0]//5,X_train_final.shape[1],rows_n))\n",
    "    lstm_rnn = build_lstm_rnn((X_train_super_final.shape[1],X_train_super_final.shape[2]) )\n",
    "    lstm_rnn.fit(X_train_super_final, y_train_super_final, epochs = 100)\n",
    "    not_final_lstm_rnn_prediction = lstm_rnn.predict(X_test_super_final)\n",
    "    lstm_rnn_prediction = np.where(not_final_lstm_rnn_prediction > 0.5, 1, 0)\n",
    "    lstm_rnn_conf = confusion_matrix(y_test_super_final,lstm_rnn_prediction)\n",
    "    lstm_rnn_plot_conf = ConfusionMatrixDisplay(lstm_rnn_conf)\n",
    "    lstm_rnn_plot_conf.plot()\n",
    "    TP=lstm_rnn_conf[1,1]\n",
    "    FP=lstm_rnn_conf[0,1]\n",
    "    TN=lstm_rnn_conf[0,0]\n",
    "    FN=lstm_rnn_conf[1,0]\n",
    "    accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
    "    recall =TP/(TP+FN)\n",
    "    precision =TP/(TP+TN)\n",
    "    accuracy_list.append(accuracy)\n",
    "    recall_list.append(recall)\n",
    "    precision_list.append(precision)\n",
    "\n",
    "print(\"Accuracy List:\", accuracy_list)\n",
    "print(\"Recall List:\", recall_list)\n",
    "print(\"Precision List:\", precision_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
